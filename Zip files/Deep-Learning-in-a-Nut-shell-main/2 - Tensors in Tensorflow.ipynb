{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TensorFlow?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a4/TensorFlowLogo.png)\n",
    "\n",
    "\n",
    "TensorFlow is an open source software library released in 2015 by Google to make it easier for developers to design, build, and train deep learning models. TensorFlow originated as an internal library that Google developers used to build models in-house, and we expect additional functionality to be added to the open source version as they are tested and vetted in the internal flavor. Although TensorFlow is only one of several options available to developers, we choose to use it here because of its thoughtful design and ease of use. We’ll briefly compare TensorFlow to alternatives in the next section.\n",
    "\n",
    "At a high level, TensorFlow is a Python library that allows users to express arbitrary computation as a graph of data flows. Nodes in this graph represent mathematical operations, whereas edges represent data that is communicated from one node to another. Data in TensorFlow are represented as tensors, which are multidimensional arrays. Although this framework for thinking about computation is valuable in many different fields, TensorFlow is primarily used for deep learning in practice and research.\n",
    "\n",
    "\n",
    "There are 2 versions of Tensorflow running popularly, \n",
    "\n",
    "   i.e.  Tensorflow 1.X    &   Tensorflow 2.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Installation of Tensorflow 1.x\n",
    "\n",
    "  $ conda create -n new_env python==3.6.9      #  Creating a new conda environment\n",
    "\n",
    "  $ source activate new_env                    #  activating New environment; remove source Windows machine\n",
    "\n",
    "  $ pip install numpy pandas matplotlib scipy seaborn scikit-learn jupyter notebook flask\n",
    "\n",
    "  $ pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importing Tensorflow and getting version\n",
    "\n",
    "import tensorflow as tf              #  'tf' is alias here \n",
    "\n",
    "tf.__version__                       #  This returns version of Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning a Variable in Tensorflow 1.X  and  calling  it\n",
    "\n",
    "tf.constant(value)   ->  This is used to create a Constant variable in Tensorflow, which cant be modified \n",
    "                               after defining it.\n",
    "\n",
    "tf.placeholder(tf.dtype)  ->  This is used to create a placeholder variable with given dtype and we used to \n",
    "                                assign values in a placeholder variable in runtime.\n",
    "                                   \n",
    "tf.Variable(value)  ->  This is used to create a Normal variable, which can be modified or upgraded further.\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##########   ############   Concept of Sessions  in Tensorflow 1.x   ##############    #########\n",
    "    \n",
    " =>  In Tensorflow 1.x, we need to create a sessions first, which checks for the required available\n",
    "       Computational resources likes as CPU, GPU, RAM etc. .\n",
    "     If all those computational resources are available in your system, then only sessions allows for further \n",
    "         executions. \n",
    "            \n",
    " =>  We cant print or acces some Tensorflow variables or objects just by calling with their names.\n",
    "      Calling them without sessions, always gives error.\n",
    "    \n",
    "      i.e.  variable = tf.constant('sudh')\n",
    "            print(variable)\n",
    "            \n",
    " =>  We need to write all codes for execution inside this session itself, otherwise it will not be executed.\n",
    "\n",
    " =>  Code to create a session and run the code ...\n",
    "\n",
    "         >>>  with tf.session() as sess:\n",
    "                  output = sess.run()\n",
    "                  print (output)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python program to create and call a Tensorflow variable\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called Tensor\n",
    "x = tf.constant(5)                          #  This is a Constant variable            \n",
    "y = tf.Variable(8)                          #  This is a Normal variable\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        # Run the tf.constant operation in the session\n",
    "        output = sess.run(x)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concepts of Tensors\n",
    "\n",
    "Tensorflow is built on top of Numpy library, so each and every object in tensorflow is a Tensor or Numpy n-d array.\n",
    " \n",
    "Patterns, Pictures, Images, Texts, speeches, logo, digits, numbers, variables all are Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ################    ###########   Tensors of different dimensions   ###########    #################\n",
    "    \n",
    " >>>  A = tf.constant(1234)                               #  A is a 0-dimensional int32 tensor\n",
    " \n",
    " >>>  B = tf.constant([123,456,789])                      #  B is a 1-dimensional int32 tensor\n",
    "\n",
    " >>>  C = tf.constant([[123,456,789], [222,333,444]])     # C is a 2-dimensional int32 tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       #############    ###########   Creating a placeholder Variable    ############    ############## \n",
    "    \n",
    " =>  tf.placeholder(tf.dtype)   ->  This is used to create a Placeholder variable with given dtype.\n",
    "\n",
    " =>  feed_dict = {key: value}   ->  We uses feed_dict{} for assigning values to Placeholder variables in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating a Placeholder variable in Tensorflow\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'sudh'})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign multiple values using this Placeholder variable.\n",
    "\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output_x = sess.run(x,feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    output_y = sess.run(y, feed_dict={x: 'Test String', y: 123, z:45.67})\n",
    "    print(output_x)\n",
    "    print(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ** Note - If the data passed to the `feed_dict` doesn’t match the tensor type and can’t be cast into\n",
    "               the tensor type, you’ll get the error `“ValueError: invalid literal for...”`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  tf.add(x, y)          ->  This medhod is used to add both x and y tensors.\n",
    "    \n",
    " =>  tf.subtract(x, y)     ->  This method is used to subtract x and y tensors.\n",
    "\n",
    " =>  tf.multiply(x, y)     ->  This method is used to multiply both tensors. \n",
    "    \n",
    " ** Note **  ->  These mathematical operations should be used for 2 numbers, 2 Tensors or 1 number and 1 Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Performing mathematical operations over Tensors and Numbers\n",
    "\n",
    "x = tf.add(5, 2)  \n",
    "y = tf.add(tf.constant(5), tf.constant(4))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ############   #############   Casting the dtypes of Tensors    ############   ##############\n",
    "    \n",
    " =>  Some mathematical operations requires same dtype, otherwise it raises an Error.   \n",
    "    \n",
    " =>  tf.cast(Tensor, tf.dtype)    ->  This is used to cast the dtype of Tensor into a given dtype.\n",
    "\n",
    " e.g.  >>> tf.cast(tf.constant(5.0), tf.int32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #############    ############   Normal variables in Tensorflow    #############     ################\n",
    "    \n",
    " =>  Normal variables are such variables, which can be modified after their creation.\n",
    "\n",
    " =>  tf.Variable() is used to create a Normal variable in Tensorflow.\n",
    "    \n",
    " =>  init = tf.global_variables_initializer() is must to be placed before creating session.\n",
    "\n",
    " =>  In session, we run this 'init' class.\n",
    "    \n",
    "    \n",
    "    e.g.  x = tf.Variable(5)\n",
    "\n",
    "          init = tf.global_variables_initializer()\n",
    "          with tf.Session() as sess:\n",
    "              sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python code to create a Normal variable in Tensorflow\n",
    "\n",
    "x = tf.Variable(5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.truncated_normal()  -> This is used to create a Normal distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow Linear Functions\n",
    "\n",
    "The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as:\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a4d8b3_linear-equation/linear-equation.gif)\n",
    "\n",
    "Here, **W** is a matrix of the weights connecting two layers. The output **y**, the input **x**, and the biases **b** are all vectors.\n",
    "\n",
    "### Weights and Bias in TensorFlow\n",
    "\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out `tf.placeholder()` and `tf.constant()`, since those Tensors can't be modified. This is where `tf.Variable` class comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 120\n",
    "n_cols = 5\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  tf.zeros(shape=())  ->  This is used to create a Tensor with all 0's and inmm a given shape.\n",
    "    \n",
    " =>  tf.ones(shape=())   ->  This is used to create a Tensor with all 1's and in a given shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Softmax\n",
    "\n",
    "The softmax function squashes it's inputs, typically called **logits** or **logit scores**, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png)\n",
    "\n",
    "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n",
    "\n",
    "Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.nn.softmax([2.0, 1.0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python code to calculate softmax function \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run_2():\n",
    "    output = None\n",
    "    logit_data = [19,354,354,45,354,54]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)   \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "print(run_2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using `LabelBinarizer`. Check it out below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Cross Entropy\n",
    "\n",
    "In the Intro to TFLearn lesson we discussed using cross entropy as the cost function for classification with one-hot encoded labels. Again, TensorFlow has a function to do the cross entropy calculations for us.\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589b18f5_cross-entropy-diagram/cross-entropy-diagram.png)\n",
    "\n",
    "To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "\n",
    "* `tf.reduce_sum()`\n",
    "* `tf.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  tf.reduce(array_of_values)  ->  This return an aggregated sum of all values of a container.\n",
    "    \n",
    " =>  tf.log(value)               ->  This return the logarithm of given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print the cross entropy using softmax_data and one_hot_encod_label.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    output = session.run(cross_entropy, feed_dict={one_hot: one_hot_data, softmax: softmax_data})\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/tf2.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "       ##############   ###########   Installation of Tensorflow 2.x    ###########   ##############\n",
    "    \n",
    " =>  Create a New environment with python 3.6.9\n",
    "\n",
    "      $ conda create -n new_env python==3.6.9\n",
    "      $ source activate new_env\n",
    "    \n",
    " =>  Installing tensorflow 2.x\n",
    "\n",
    "      $ pip install tensorflow\n",
    "    \n",
    " =>  Installing some other required packages in new environment\n",
    "\n",
    "      $ pip install jupyter notebook, numpy, pandas, matplotlib, seaborn, scikit-learn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##  Python code to check CPU/GPU \n",
    "\n",
    "if tf.test.is_gpu-available():\n",
    "    \n",
    "    print ('GPU')\n",
    "    print ('GPU #0?')\n",
    "    print (var.device.endswith('GPU:0'))\n",
    "    \n",
    "else:\n",
    "    print ('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #####################   ##########   Some basic things in Tensorflow 2.x   ###########   ##################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow Methods Documentation](https://www.tensorflow.org/api_docs/python/tf/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ############    ###########    Creating a Constant in Tensorflow 2.x    ###########    #############\n",
    "    \n",
    " >>> constant = tf.constant(42, dtype = tf.int32)    #  This is a Constant Tensor with dtype 'int32' .\n",
    "\n",
    " >>> variable = tf.Variable(42)                      #  This is a  Variable Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Calling these Tensors with their name will give its signature only.\n",
    "    \n",
    "      >>> constant   ->  <tf.tensor: shape=(), dtype=int32, numpy=42> \n",
    "            \n",
    " =>  This means that we have created a Constant Tensor with value '42' and 'dtype = int32' and it is get\n",
    "       stored as a Numpy array.\n",
    "    \n",
    " =>  For accessing the value of this Constant Tensor, we call its name with reference to numpy().\n",
    "\n",
    "       i.e.,  >>> constant.numpy()  ->  42\n",
    "    \n",
    " =>  constant.shape()  ->  This gives the shape of Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Creating a Tensor with 2-d array \n",
    "\n",
    "2d_tensor = tf.tensor([[4, 2], [9, 5]])\n",
    "2d_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adding operation between 2 different tensors\n",
    "\n",
    "tensor_1 = tf.constant([[1, 2, 3], [1, 2, 3]])\n",
    "tensor_2 = tf.constant([[3,4,5], [3, 4, 5]])\n",
    "\n",
    "tensor_sum = tf.add(tensor_1 + tensor_2)\n",
    "tensor_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Creating a Tensor with Normally distributed data betwwen 0 and 10\n",
    "\n",
    "      >>>  tf.random.normal(shape=(2,2), minval=0, maxval=10, dtypes=tf.int32)\n",
    "        \n",
    " =>  Creating a Tensor with Uniformly distributed data betwwen 0 and 10\n",
    "     \n",
    "      >>>  tf.random.uniform(shape = (2,2), minval, maxval, dtypes = tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ** NOTE  =>  In Tensorflow 2.x, int32 is default dtype for Integers and tf.float32 is default dtype for Floats ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #############   ###########   Re-assigning a Normal Variable in tensorflow 2.x   ###########   #############\n",
    "    \n",
    " => variable.assign(value)  ->  This is used to re-assign some values to Variables.\n",
    "\n",
    " =>  assert() is also used for assigning values to Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ###################    ################    Reshaping a Tensor    ################    ##################\n",
    "    \n",
    " =>  Tensor can be reshaped & retain the same values which is requird for constructing the Neural networks.\n",
    "    \n",
    " =>  tf.reshape(tensor, shape=[x,y])  ->  This method is used to reshape the Tensor with given shape.\n",
    "\n",
    " ** Note **  ->  As shape,if one component is '-1', then it uderstand the shape automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         ##################     ###############   Rank of a Tensor    ################     ##################\n",
    "    \n",
    " =>  The rank of a tensor is not the same as the rank of a matrix. \n",
    "\n",
    " =>  The rank of a tensor is the number of indices required to uniquely select each element of the tensor. \n",
    "       Rank is also known as \"order\", \"degree\", or \"ndims.\" .\n",
    "        \n",
    " >>>  tf.rank(tensor)  ->  This returns the Rank of Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      #############     ##############    Casting a Tensor to a Numpy Array    ##############     ################\n",
    "    \n",
    "   >>>  tensor.numpy()  ->  This method is used to cast a Tensor to a Numpy Array."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Size of a Tensor\n",
    "\n",
    "tf.size(input = Tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Some mathematical operations over Tensors\n",
    "\n",
    "  >>> tf.square(Tensor)  ->  This method calculates the square root of tensor.\n",
    "    \n",
    "  >>> tf.exp(Tensor)   ->  This method calculates exponential of tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Elementwose operatins over Tensors\n",
    "\n",
    " >>>  Tensor [+, -, *, /] Tensor  ->  This performs addition, subtraction like operations over two given Tensors.\n",
    "\n",
    "##  Broadcasting of Tensors\n",
    "\n",
    " >>>  tensor * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " >>>  tf.transpose(Tensor)  ->  This gives the Transpose of given tensor.\n",
    "    \n",
    " >>>  tf.matmul(Tensor_x, Tensor_y)  ->  This performs matrix multiplication over 2 given tensors.\n",
    "                                           Both tensors should be compatible for Multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Casting a tensor\n",
    "\n",
    " >>> tf.cast(Tensor, dtype)   ->   This casts the given Tensor into specified dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Ragged Tensors\n",
    "\n",
    " =>  Ragged tensors are such type of Tensors, which have slices of different lengths..\n",
    "    \n",
    " =>  ragged Tensors are Nested tensors having some Tensors of different different sizes.\n",
    "    \n",
    "    e.x.  ragged = tf.ragged.constant([[9,7,4,3], [], [11, 12, 8], [3], [7, 6, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Squared Differences of Tensors\n",
    "\n",
    " >>> tf.math.squared_difference(x,y)  ->  This method returns the Squared difference of a;ll items of Tensor named X.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculation of squared differences\n",
    "\n",
    "x = [1. 2, 3, 4, 5]; y = 8\n",
    "\n",
    "sqrt_diff = tf.math.squared_difference(x,y)\n",
    "sqrt_diff\n",
    "\n",
    " =>  (1, 4, 9, 16, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting reduced_mean of Tensors\n",
    "\n",
    " >>> tf.reduce_mean(input_tensor = )  ->  This method return the reduced mean of given Tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " >>>  tf.argmax(Tensor)  ->  This return the Maxm item of Tensor.\n",
    " >>>  tf.argmin(Tensor)  ->  This return the Minm item of Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data may also be passed into the fit method as a tf.data.Dataset() iterator\n",
    "> The from_tensor_slices() method converts the NumPy arrays into a dataset\n",
    "> The batch() and shuffle() methods chained together. \n",
    "\n",
    ">Next, the map() method invokes a method on the input images, x, that randomly flips one in two of them across\n",
    "the y-axis, effectively increasing the size of the image set\n",
    "\n",
    ">Finally, the repeat() method means that the dataset will be re-fed from the beginning when its end is\n",
    "reached (continuously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
    "train_x, test_x = train_x/255.0, test_x/255.0\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_size = 10000\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
    "training_dataset = training_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "training_dataset = training_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)\n",
    "testing_dataset = training_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in the fit() function, we can pass the dataset directly in, as follows:\n",
    "model5 = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_x)//batch_size #required becuase of the repeat() on the dataset\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model5.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3603 - accuracy: 0.8916\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1780 - accuracy: 0.9455\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1355 - accuracy: 0.9585\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1149 - accuracy: 0.9635\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1004 - accuracy: 0.9681\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0906 - accuracy: 0.9719\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0801 - accuracy: 0.9748\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0747 - accuracy: 0.9759\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0686 - accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0660 - accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x171b07b5888>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(training_dataset, epochs=epochs, steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0301 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.030064156164735324, 0.99375]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(testing_dataset,steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='log/{}/'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps, validate for 3 steps\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0607 - accuracy: 0.9803 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0574 - accuracy: 0.9811 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0547 - accuracy: 0.9822 - val_loss: 0.0382 - val_accuracy: 0.9792\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0507 - accuracy: 0.9837 - val_loss: 0.0241 - val_accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0498 - accuracy: 0.9839 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0449 - accuracy: 0.9853 - val_loss: 0.0312 - val_accuracy: 0.9792\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0450 - accuracy: 0.9855 - val_loss: 0.0705 - val_accuracy: 0.9792\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0423 - accuracy: 0.9861 - val_loss: 0.0372 - val_accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.0186 - val_accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x171de9e6388>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(training_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=testing_dataset,\n",
    "          validation_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0150 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015026384776138001, 0.99375]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(testing_dataset,steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The Keras API in TensorFlow has the ability to save and restore models easily. This is done as follows, and saves the model in the current directory. Of course, a longer path may be passed here:\n",
    "\n",
    "#### Saving a model\n",
    "    \n",
    "`model.save('./model_name.h5')`\n",
    "\n",
    ">This will save the model architecture, its weights, its training state (loss, optimizer), and the state of the optimizer, so that you can carry on training the model from where you left off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Loading a saved model is done as follows. Note that if you have compiled your model, the load will compile your model using the saved training configuration:\n",
    "\n",
    "#### Loding a model\n",
    "\n",
    "`from tensorflow.keras.models import load_model\n",
    "new_model = load_model('./model_name.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">It is also possible to save just the model weights and load them with this (in which case, you must build your architecture to load the weights into):\n",
    "\n",
    "#### Saving the model weights only\n",
    "    \n",
    "    `model.save_weights('./model_weights.h5')`\n",
    "    \n",
    ">Then use the following to load it:\n",
    "\n",
    "#### Loding the weights\n",
    "    \n",
    "    `model.load_weights('./model_weights.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras datasets\n",
    "\n",
    ">The following datasets are available from within Keras: boston_housing, cifar10, cifar100, fashion_mnist, imdb, mnist,and reuters.\n",
    "\n",
    ">They are all accessed with the function.\n",
    "\n",
    "`load_data()`  \n",
    "\n",
    ">For example, to load the fashion_mnist dataset, use the following:\n",
    "\n",
    "`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NumPy arrays with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "number_items = 11\n",
    "number_list1 = np.arange(number_items)\n",
    "number_list2 = np.arange(number_items,number_items*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datasets, using the from_tensor_slices() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an iterator on it using the make_one_shot_iterator() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using them together, with the get_next method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for item in number_list1_dataset:\n",
    "    number = iterator.get_next().numpy()\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note that executing this code twice in the same program run will raise an error because we are using a one-shot iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's also possible to access the data in batches() with the batch method. Note that the first argument is the number of elements to put in each batch and the second is the self-explanatory drop_remainder argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[ 9 10]\n"
     ]
    }
   ],
   "source": [
    "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1).batch(3, drop_remainder = False)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)\n",
    "for item in number_list1_dataset:\n",
    "    number = iterator.get_next().numpy()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is also a zip method, which is useful for presenting features and labels together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=string, numpy=b'a'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=string, numpy=b'e'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=string, numpy=b'i'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=4>, <tf.Tensor: shape=(), dtype=string, numpy=b'o'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=string, numpy=b'u'>)\n"
     ]
    }
   ],
   "source": [
    "data_set1 = [1,2,3,4,5]\n",
    "data_set2 = ['a','e','i','o','u']\n",
    "data_set1 = tf.data.Dataset.from_tensor_slices(data_set1)\n",
    "data_set2 = tf.data.Dataset.from_tensor_slices(data_set2)\n",
    "zipped_datasets = tf.data.Dataset.zip((data_set1, data_set2))\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
    "for item in zipped_datasets:\n",
    "    number = iterator.get_next()\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can concatenate two datasets as follows, using the concatenate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(29, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n",
      "tf.Tensor(37, shape=(), dtype=int32)\n",
      "tf.Tensor(41, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "datas1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
    "datas2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
    "datas3 = datas1.concatenate(datas2)\n",
    "print(datas3)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(datas3)\n",
    "for i in range(14):\n",
    "    number = iterator.get_next()\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also do away with iterators altogether, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(29, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n",
      "tf.Tensor(37, shape=(), dtype=int32)\n",
      "tf.Tensor(41, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(29, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n",
      "tf.Tensor(37, shape=(), dtype=int32)\n",
      "tf.Tensor(41, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "for e in range(epochs):\n",
    "    for item in datas3:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using comma-separated value (CSV)files with datasets.\n",
    "\n",
    ">CSV files are a very popular method of storing data. TensorFlow 2 contains flexible methods for dealing with them. \n",
    "\n",
    ">The main method here is tf.data.experimental.CsvDataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">With the following arguments, our dataset will consist of two items taken from each row of the\n",
    "filename file, both of the float type, with the first line of the file ignored and columns 1 and 2 used\n",
    "(column numbering is, of course, 0-based):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = [\"./size_1000.csv\"]\n",
    "record_defaults = [tf.float32] * 2 # two required float columns\n",
    "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])\n",
    "for item in data_set:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #CSV example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In this example, and with the following arguments, our dataset will consist of one required float,\n",
    "one optional float with a default value of 0.0, and an int, where there is no header in the CSV file and\n",
    "only columns 1, 2, and 3 are imported:\n",
    "#file Chapter_2.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float32, numpy=428000.0>, <tf.Tensor: shape=(), dtype=float32, numpy=555.0>, <tf.Tensor: shape=(), dtype=int32, numpy=42>)\n",
      "(<tf.Tensor: shape=(), dtype=float32, numpy=-5.3>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=int32, numpy=69>)\n"
     ]
    }
   ],
   "source": [
    "filename = \"mycsvfile.txt\"\n",
    "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
    "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
    "for item in data_set:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #CSV example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 23.4  Abc.co.uk\n",
      "98.7 56.8  Xyz.com\n",
      "34.2 68.1  Pqr.net\n"
     ]
    }
   ],
   "source": [
    "#For our final example, our dataset will consist of two required floats and a required string, where the\n",
    "#CSV file has a header variable:\n",
    "filename = \"file1.txt\"\n",
    "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
    "for item in dataset:\n",
    "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TFRecord format is a binary file format. For large files, it is a good choice because binary files take up less disc space, take less time to copy, and can be read very efficiently from the disc. All this can have a significant effect on the efficiency of your data pipeline and thus, the training time of your model. The format is also optimized in a\n",
    "variety of ways for use with TensorFlow. It is a little complex because data has to be converted into\n",
    "the binary format prior to storage and decoded when read back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #TFRecord example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">A TFRecord file is a sequence of binary strings, its structure must be specified prior to\n",
    "saving so that it can be properly written and subsequently read back.\n",
    "\n",
    ">TensorFlow has two structures for this, \n",
    "\n",
    "`tf.train.Example and tf.train.SequenceExample. `\n",
    "\n",
    ">We have to store each sample of your data in one of these structures, then serialize it, and use `tf.python_io.TFRecordWriter` to save it to disk.\n",
    "\n",
    ">In the next example, \n",
    "the  data, is first converted to the binary format and then saved to disc.\n",
    "\n",
    ">A feature is a dictionary containing the data that is passed to tf.train.Example prior to serialization and saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "data = np.array([10.,11.,12.,13.,14.,15.])\n",
    "def npy_to_tfrecords(fname,data):\n",
    "    writer = tf.io.TFRecordWriter(fname)\n",
    "    feature={}\n",
    "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "    writer.write(serialized)\n",
    "    writer.close()\n",
    "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The code to read the record back is as follows. \n",
    "\n",
    ">A parse_function function is constructed that decodes the dataset read back from the file. This requires a dictionary (keys_to_features) with the same name and structure as the saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10. 11. 12. 13. 14. 15.], shape=(6,), dtype=float32)\n",
      "[10. 11. 12. 13. 14. 15.]\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "data_set = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features['data']\n",
    "data_set = data_set.map(parse_function)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(data_set)\n",
    "# array is retrieved as one item\n",
    "item = iterator.get_next()\n",
    "print(item)\n",
    "print(item.numpy())\n",
    "print(item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFRecord example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './students.tfrecords'\n",
    "dataset = {\n",
    "'ID': 61553,\n",
    "'Name': ['Jones', 'Felicity'],\n",
    "'Scores': [45.6, 97.2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using this, we can construct a tf.train.Example class, again using the `Feature()` method. Note how we have to encode our string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[dataset['ID']]))\n",
    "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in dataset['Name']]))\n",
    "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=dataset['Scores']))\n",
    "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Serializing and writing this record to disc is the same as TFRecord example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_rec = tf.io.TFRecordWriter(filename)\n",
    "writer_rec.write(example.SerializeToString())\n",
    "writer_rec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To read this back, we just need to construct our parse_function function to reflect the structure of the record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
    "    'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
    "    'Scores':tf.io.VarLenFeature(dtype = tf.float32)}\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_set.map(parse_function)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "items = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record is retrieved as one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int64, numpy=61553>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000171A13D64C8>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000171AC2E0BC8>)\n"
     ]
    }
   ],
   "source": [
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we can extract our data from item (note that the string must be decoded (from bytes) where the default for our Python 3 is utf8). Note also that the string and\n",
    "the array of floats are returned as sparse arrays, and to extract them from the record, we use the sparse array value method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  61553\n",
      "Name: Jones , Felicity\n",
      "Scores:  [45.6 97.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"ID: \",item[0].numpy())\n",
    "name = item[1].values.numpy()\n",
    "name1= name[0].decode()\n",
    "name2 = name[1].decode('utf8')\n",
    "print(\"Name:\",name1,\",\",name2)\n",
    "print(\"Scores: \",item[2].values.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">One-hot encoding (OHE) is where a tensor is constructed from the data labels with a 1 in each of\n",
    "the elements corresponding to a label's value, and 0 everywhere else; that is, one of the bits in the\n",
    "tensor is hot (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this example, we are converting a decimal value of 7 to a one-hot encoded value of 0000000100 using\n",
    "\n",
    "`the tf.one_hot() method:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 is  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] when one-hot encoded with a depth of 10\n"
     ]
    }
   ],
   "source": [
    "z = 7\n",
    "z_train_ohe = tf.one_hot(z, depth=10).numpy()\n",
    "print(z, \"is \",z_train_ohe,\"when one-hot encoded with a depth of 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using the fashion MNIST dataset.\n",
    "\n",
    ">The original labels are integers from 0 to 9, so, for example, a label of 5 becomes 0000010000 when onehot encoded, but note the difference between the index and the label stored at that index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import fashion_mnist\n",
    "\n",
    "width, height, = 28,28\n",
    "# total classes\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split feature training set into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 50000\n",
    "(y_train, y_valid) = y_train[:split], y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encode the labels using TensorFlow then convert back to numpy for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train_ohe = tf.one_hot(y_train, depth=n_classes).numpy()\n",
    "y_valid_ohe = tf.one_hot(y_valid, depth=n_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=n_classes).numpy()\n",
    "\n",
    "# show difference between the original label and a one-hot-encoded label\n",
    "i=8\n",
    "print(y_train[i]) # 'ordinary' number value of label at index i=8 is 5\n",
    "# note the difference between the index of 8 and the label at that index which is 5\n",
    "print(y_train_ohe[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
